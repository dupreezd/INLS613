{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8684\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import csv\n",
    "\n",
    "text = pd.read_csv(\"all_opinions.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "issues = pd.read_csv(\"database.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "privacyCaseSet = set([])\n",
    "#for each row in the issues column of issues csv\n",
    "issuesFile = open(\"database.csv\")\n",
    "issuesReader = csv.reader(issuesFile, delimiter=',')\n",
    "for row in issuesReader:\n",
    "    if row[39]:\n",
    "    #if the integers value matches 30040, 50010, 50020, 50030, or 50040 (floats)\n",
    "        privacyCaseSet.add(row[0])\n",
    "        #add case_id value of same row to set\n",
    "#print(privacyCaseSet)\n",
    "print(len(privacyCaseSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14948\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "opinionsFile = open(\"all_opinions.csv\")\n",
    "opinionsReader = csv.reader(opinionsFile, delimiter=',')\n",
    "dictionary = {}\n",
    "count = 0\n",
    "\n",
    "for r in opinionsReader:   \n",
    "    \n",
    "    if r[9] in privacyCaseSet:\n",
    "        #count += 1 \n",
    "        if r[9]+'-01' in dictionary.keys():\n",
    "            if r[9]+'-02' in dictionary.keys():\n",
    "                if r[9]+'-03' in dictionary.keys():\n",
    "                    if r[9]+'-04' in dictionary.keys():\n",
    "                        dictionary[r[9]+'-05'] = r[-1]\n",
    "                    else: dictionary[r[9]+'-04'] = r[-1]\n",
    "                else: dictionary[r[9]+'-03'] = r[-1]\n",
    "            else: dictionary[r[9]+'-02'] = r[-1]\n",
    "        else: dictionary[r[9]+'-01'] = r[-1]     \n",
    "            \n",
    "\n",
    "print(len(dictionary))\n",
    "print(count)\n",
    "\n",
    "with open('dict2.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "   # writer.writerow(['id','text'])\n",
    "    for key, value in dictionary.items():\n",
    "        writer.writerow([key, value])\n",
    "        \n",
    "test = pd.read_csv('dict2.csv',error_bad_lines=False,encoding='ISO-8859-1',names=['id','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "testList = []\n",
    "\n",
    "for entry in dictionary.values():\n",
    "    testTokens = nltk.word_tokenize(entry)\n",
    "    if \"privacy\" in testTokens:\n",
    "        testList.append(entry)\n",
    "\n",
    "print(len(testList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 1000\n",
    "tf_vectorizer = CountVectorizer(max_df=.2, min_df=.05, max_features=num_features, stop_words='english')\n",
    "\n",
    "data_samples = testList\n",
    "\n",
    "tf_data_samples = tf_vectorizer.fit_transform(data_samples) # Tokenizing and getting the feature counts. \n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "[('ordinance', 727.7025162418108), ('election', 531.895834257017), ('commission', 381.2856354490053), ('commercial', 341.35215194130626), ('organization', 325.37039637626174), ('restriction', 267.7239199316647), ('appellants', 249.03711712503545), ('violence', 225.38228988831182)]\n",
      "Topic 1:\n",
      "[('religious', 755.9405506382493), ('surveillance', 656.011826084225), ('electronic', 504.13316079720755), ('conversations', 430.9634610781482), ('telephone', 414.1074493191385), ('katz', 336.41918521627093), ('title', 290.90960040273814), ('communications', 253.10872555167754)]\n",
      "Topic 2:\n",
      "[('vehicle', 736.4662688039926), ('stop', 656.9041976355785), ('terry', 597.0642036126378), ('ct', 480.45165843932836), ('traffic', 429.85985815380104), ('detention', 335.2403257802848), ('border', 296.75537178098506), ('automobile', 295.7114040743438)]\n",
      "Topic 3:\n",
      "[('cite', 515.6605453634814), ('____', 510.47115049990157), ('blood', 488.5249605513684), ('___', 443.4483559166269), ('alcohol', 321.223345869584), ('tion', 298.7114191394623), ('tests', 283.8773730742674), ('ing', 268.0337246375921)]\n",
      "Topic 4:\n",
      "[('damages', 1068.5247422168848), ('liability', 524.6077082326217), ('immunity', 409.8885506002474), ('publication', 358.70568751904364), ('injury', 344.3542774247128), ('plaintiff', 342.74345919476013), ('false', 331.3495419041059), ('libel', 264.3173359924719)]\n",
      "Topic 5:\n",
      "[('committee', 954.9949777878242), ('president', 764.7840691566674), ('documents', 644.4792984603735), ('foreign', 498.43753695869276), ('exemption', 493.6841035889119), ('executive', 484.94184843611885), ('bank', 435.2968973042985), ('tax', 412.2844277877253)]\n",
      "Topic 6:\n",
      "[('child', 834.5022952284565), ('children', 443.3975045395648), ('substantive', 441.1774690925857), ('marriage', 390.9311303532876), ('habeas', 378.11434332967525), ('sex', 337.34351720766244), ('women', 334.29120640174807), ('death', 277.40376171690104)]\n",
      "Topic 7:\n",
      "[('appellant', 834.8682811683599), ('solicitation', 430.9510108207636), ('client', 313.49655804221976), ('lawyer', 289.64522020178015), ('mail', 268.8956672466568), ('commercial', 264.796222378803), ('advertising', 257.1939419045084), ('lawyers', 234.5233355028286)]\n",
      "Topic 8:\n",
      "[('grand', 660.6149843784998), ('witness', 613.3772590149381), ('exclusionary', 514.7581410824506), ('accused', 470.2621828833993), ('incrimination', 353.5011330229072), ('witnesses', 312.0917345192706), ('weeks', 296.95525654755136), ('interrogation', 275.2393258739892)]\n",
      "Topic 9:\n",
      "[('employees', 780.6049014957844), ('union', 505.0594588328615), ('prison', 415.1024082771604), ('employee', 402.387853196848), ('labor', 373.6529278295677), ('employer', 322.88348453385754), ('inspection', 230.4302741594035), ('employment', 199.6376285169338)]\n",
      "Topic 10:\n",
      "[('obscene', 440.796628199521), ('plurality', 429.4424969970551), ('georgia', 267.12633425983165), ('sexual', 213.04456073664383), ('distribution', 182.30828432139094), ('stanley', 162.47758943194896), ('sell', 160.79017686009072), ('books', 148.5145458302867)]\n",
      "Topic 11:\n",
      "[('magistrate', 578.1133107544159), ('automobile', 557.5852559181731), ('apartment', 529.3138969668123), ('contraband', 482.6458007438731), ('container', 381.4448086523563), ('miranda', 310.9974800615144), ('narcotics', 308.0353362221137), ('items', 301.4611272152399)]\n",
      "Topic 12:\n",
      "[('appellate', 230.4618653979707), ('error', 221.16305097245692), ('columbia', 181.09239226013892), ('contempt', 177.89681724315622), ('forfeiture', 171.1331528626693), ('merits', 168.4166214164007), ('prosecutor', 156.7013403310475), ('pending', 154.7822909761598)]\n",
      "Topic 13:\n",
      "[('abortion', 1248.5296293090373), ('medical', 733.5792046717105), ('physician', 552.1575879635245), ('woman', 532.7731365319636), ('roe', 439.4087973555528), ('injunction', 372.00565295671737), ('patient', 369.2841622059394), ('pregnancy', 328.87327817220836)]\n",
      "Topic 14:\n",
      "[('school', 1722.2669417146915), ('drug', 802.5565102224922), ('schools', 713.4711020590624), ('program', 466.5967941730064), ('education', 446.9626829410408), ('students', 439.0111529819441), ('drugs', 371.4814796642004), ('student', 332.4953455510563)]\n"
     ]
    }
   ],
   "source": [
    "num_topics = 15\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=500, learning_method='online', learning_offset=50.,random_state=1).fit(tf_data_samples)\n",
    "\n",
    "lda.score(tf_data_samples)\n",
    "\n",
    "from IPython.display import display\n",
    "def print_topics(model, vectorizer, top_n=8):    #### This is the code to print the topics. top_n can be changed. \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print_topics(lda,tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore fragment the data, optimize variables see what we get\n",
    "#test our best results with word intrusion\n",
    "#see which topics are different between time periods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
