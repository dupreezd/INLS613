{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "text = pd.read_csv(\"all_opinions.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "issues = pd.read_csv(\"database.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "privacyCaseSet = set([])\n",
    "#for each row in the issues column of issues csv\n",
    "issuesFile = open(\"database.csv\")\n",
    "issuesReader = csv.reader(issuesFile, delimiter=',')\n",
    "for row in issuesReader:\n",
    "    if row[39]:\n",
    "    #if the integers value matches 30040, 50010, 50020, 50030, or 50040 (floats)\n",
    "        privacyCaseSet.add(row[0])\n",
    "        #add case_id value of same row to set\n",
    "#print(privacyCaseSet)\n",
    "#print(len(privacyCaseSet))\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "opinionsFile = open(\"all_opinions.csv\")\n",
    "opinionsReader = csv.reader(opinionsFile, delimiter=',')\n",
    "dictionary = {}\n",
    "count = 0\n",
    "\n",
    "for r in opinionsReader:   \n",
    "    \n",
    "    if r[9] in privacyCaseSet:\n",
    "        #count += 1 \n",
    "        if r[9]+'-01' in dictionary.keys():\n",
    "            if r[9]+'-02' in dictionary.keys():\n",
    "                if r[9]+'-03' in dictionary.keys():\n",
    "                    if r[9]+'-04' in dictionary.keys():\n",
    "                        dictionary[r[9]+'-05'] = r[-1]\n",
    "                    else: dictionary[r[9]+'-04'] = r[-1]\n",
    "                else: dictionary[r[9]+'-03'] = r[-1]\n",
    "            else: dictionary[r[9]+'-02'] = r[-1]\n",
    "        else: dictionary[r[9]+'-01'] = r[-1]     \n",
    "            \n",
    "\n",
    "#print(len(dictionary))\n",
    "#print(count)\n",
    "\n",
    "with open('dict2.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "   # writer.writerow(['id','text'])\n",
    "    for key, value in dictionary.items():\n",
    "        writer.writerow([key, value])\n",
    "        \n",
    "test = pd.read_csv('dict2.csv',error_bad_lines=False,encoding='ISO-8859-1',names=['id','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data by years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourSix = {}\n",
    "sevenNine = {}\n",
    "nineTwenty = {}\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    if key.startswith(str(194)):\n",
    "        fourSix.update({key : value})\n",
    "    if key.startswith(str(195)):\n",
    "        fourSix.update({key : value})\n",
    "    if key.startswith(str(196)):\n",
    "        fourSix.update({key : value})\n",
    "    if key.startswith(str(197)):\n",
    "        sevenNine.update({key : value})\n",
    "    if key.startswith(str(198)):\n",
    "        sevenNine.update({key : value})\n",
    "    if key.startswith(str(199)):\n",
    "        nineTwenty.update({key : value})\n",
    "    if key.startswith(str(200)):\n",
    "        nineTwenty.update({key : value})\n",
    "    if key.startswith(str(201)):\n",
    "        nineTwenty.update({key : value})\n",
    "        \n",
    "testList1 = []\n",
    "\n",
    "for entry1 in fourSix.values():\n",
    "    testTokens1 = nltk.word_tokenize(entry1)\n",
    "    if \"privacy\" in testTokens1:\n",
    "        testList1.append(entry1)\n",
    "\n",
    "#print(len(testList1))\n",
    "\n",
    "\n",
    "testList2 = []\n",
    "\n",
    "for entry2 in sevenNine.values():\n",
    "    testTokens2 = nltk.word_tokenize(entry2)\n",
    "    if \"privacy\" in testTokens2:\n",
    "        testList2.append(entry2)\n",
    "\n",
    "#print(len(testList2))\n",
    "\n",
    "\n",
    "testList3 = []\n",
    "\n",
    "for entry3 in nineTwenty.values():\n",
    "    testTokens3 = nltk.word_tokenize(entry3)\n",
    "    if \"privacy\" in testTokens3:\n",
    "        testList3.append(entry3)\n",
    "\n",
    "#print(len(testList3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics = 15\n",
    "#lda = LatentDirichletAllocation(n_components=num_topics, max_iter=500, learning_method='online', learning_offset=50.,random_state=1).fit(tf_data_samples)\n",
    "\n",
    "#lda.score(tf_data_samples)\n",
    "\n",
    "from IPython.display import display\n",
    "def print_topics(model, vectorizer, top_n=10):    #### This is the code to print the topics. top_n can be changed. \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "#print_topics(lda,tf_vectorizer)\n",
    "\n",
    "num_features = 100\n",
    "tf_vectorizer = CountVectorizer(max_df=.25, min_df=.05, max_features=num_features, stop_words='english')\n",
    "\n",
    "yearDfs2 = [testList1, testList2, testList3]\n",
    "ldaList = []\n",
    "\n",
    "for year in yearDfs2:\n",
    "    #n_samples = 200\n",
    "    #data_samples = random.sample(year.text.tolist(), n_samples)\n",
    "    data_samples = year\n",
    "    tf_data_samples = tf_vectorizer.fit_transform(data_samples) \n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    num_topics = 8\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=500, learning_method='online', learning_offset=50.,random_state=1).fit(tf_data_samples)\n",
    "    lda.score(tf_data_samples)\n",
    "    \n",
    "    ldaList.append(lda)\n",
    "    \n",
    "#   print(lda.score(tf_data_samples))\n",
    "#   print_topics(lda,tf_vectorizer)\n",
    "    \n",
    "#    print()\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Intrustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "0    ___\n",
      "1    notice\n",
      "2    testing\n",
      "3    primary\n",
      "4    religious\n",
      "5    ct\n",
      "6    family\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 1:\n",
      "0    notice\n",
      "1    clinic\n",
      "2    ct\n",
      "3    false\n",
      "4    vehicle\n",
      "5    arrested\n",
      "6    compelling\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 2:\n",
      "0    ordinance\n",
      "1    occupant\n",
      "2    program\n",
      "3    education\n",
      "4    insurance\n",
      "5    plaintiffs\n",
      "6    abortion\n",
      "\n",
      "1\n",
      "success!\n",
      "Topic 3:\n",
      "0    substantive\n",
      "1    ment\n",
      "2    000\n",
      "3    abuse\n",
      "4    occupant\n",
      "5    employee\n",
      "6    defense\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 4:\n",
      "0    occupant\n",
      "1    schools\n",
      "2    students\n",
      "3    tradition\n",
      "4    disclosure\n",
      "5    washington\n",
      "6    blood\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 5:\n",
      "0    terry\n",
      "1    religious\n",
      "2    speech\n",
      "3    reading\n",
      "4    000\n",
      "5    arrested\n",
      "6    testing\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 6:\n",
      "0    trust\n",
      "1    woman\n",
      "2    parents\n",
      "3    notice\n",
      "4    dog\n",
      "5    liability\n",
      "6    student\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 7:\n",
      "0    damages\n",
      "1    driving\n",
      "2    traffic\n",
      "3    religious\n",
      "4    entry\n",
      "5    privilege\n",
      "6    op\n",
      "\n",
      "1\n",
      "fail :(\n",
      "[0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Topic 0:\n",
      "0    warrants\n",
      "1    liability\n",
      "2    injunction\n",
      "3    employee\n",
      "4    thomas\n",
      "5    students\n",
      "6    2012\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 1:\n",
      "0    patients\n",
      "1    privilege\n",
      "2    commercial\n",
      "3    injury\n",
      "4    foreign\n",
      "5    prison\n",
      "6    students\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 2:\n",
      "0    patients\n",
      "1    program\n",
      "2    communication\n",
      "3    reading\n",
      "4    agency\n",
      "5    dog\n",
      "6    religious\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 3:\n",
      "0    king\n",
      "1    immunity\n",
      "2    000\n",
      "3    entry\n",
      "4    tests\n",
      "5    patients\n",
      "6    incident\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 4:\n",
      "0    exigent\n",
      "1    disclosure\n",
      "2    terry\n",
      "3    expression\n",
      "4    ordinance\n",
      "5    plaintiffs\n",
      "6    child\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 5:\n",
      "0    qualified\n",
      "1    procedural\n",
      "2    speech\n",
      "3    false\n",
      "4    king\n",
      "5    terry\n",
      "6    abuse\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 6:\n",
      "0    arrested\n",
      "1    testing\n",
      "2    traffic\n",
      "3    contraband\n",
      "4    prosecution\n",
      "5    warrants\n",
      "6    slip\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 7:\n",
      "0    2012\n",
      "1    immunity\n",
      "2    men\n",
      "3    ban\n",
      "4    sex\n",
      "5    automobile\n",
      "6    notice\n",
      "\n",
      "1\n",
      "fail :(\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Topic 0:\n",
      "0    clinic\n",
      "1    commercial\n",
      "2    false\n",
      "3    communication\n",
      "4    expression\n",
      "5    miranda\n",
      "6    injunction\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 1:\n",
      "0    injury\n",
      "1    liability\n",
      "2    restriction\n",
      "3    defendants\n",
      "4    damages\n",
      "5    error\n",
      "6    notice\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 2:\n",
      "0    employee\n",
      "1    religious\n",
      "2    trust\n",
      "3    women\n",
      "4    expression\n",
      "5    abortion\n",
      "6    compelling\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 3:\n",
      "0    ill\n",
      "1    substantive\n",
      "2    patient\n",
      "3    washington\n",
      "4    death\n",
      "5    restriction\n",
      "6    plaintiffs\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 4:\n",
      "0    records\n",
      "1    agency\n",
      "2    prosecution\n",
      "3    disclosure\n",
      "4    privilege\n",
      "5    foreign\n",
      "6    ordinance\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 5:\n",
      "0    automobile\n",
      "1    occupant\n",
      "2    vehicle\n",
      "3    suspicion\n",
      "4    traffic\n",
      "5    blood\n",
      "6    commercial\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 6:\n",
      "0    student\n",
      "1    child\n",
      "2    education\n",
      "3    expression\n",
      "4    students\n",
      "5    suspicion\n",
      "6    parents\n",
      "\n",
      "1\n",
      "fail :(\n",
      "Topic 7:\n",
      "0    ___\n",
      "1    communication\n",
      "2    substantive\n",
      "3    ____\n",
      "4    2012\n",
      "5    2011\n",
      "6    sex\n",
      "\n",
      "1\n",
      "success!\n",
      "[0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[[0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "# 1# explore fragment the data, optimize variables see what we get\n",
    "#test our best results with word intrusion\n",
    "#see which topics are different between time periods\n",
    "\n",
    "resultsList = []\n",
    "for ldaItem in ldaList:\n",
    "    successList = []\n",
    "    for idx1, topic1 in enumerate(ldaItem.components_):\n",
    "            print(\"Topic %d:\" % (idx1))\n",
    "            array = [(tf_vectorizer.get_feature_names()[j])\n",
    "                            for j in topic1.argsort()[:-10 - 1:-1]]\n",
    "\n",
    "            cArray = [(tf_vectorizer.get_feature_names()[j])\n",
    "                             for j in topic1.argsort()[:-50 - 1:-1]]\n",
    "\n",
    "            set1 = []\n",
    "            for c in array:\n",
    "                set1.append(c)\n",
    "\n",
    "            array = random.sample(array, 6)\n",
    "\n",
    "            intruder = \"?\"\n",
    "\n",
    "            for idx2, topic2 in enumerate(ldaItem.components_):\n",
    "                if idx2 == idx1:\n",
    "                    continue\n",
    "                else:\n",
    "                    bArray = [(tf_vectorizer.get_feature_names()[k])\n",
    "                            for k in topic2.argsort()[:-10 - 1:-1]]\n",
    "                    bArray = random.sample(bArray, 1)\n",
    "                    if bArray in set1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        array.append(bArray[0])\n",
    "                        intruder = bArray[0]\n",
    "                        break\n",
    "\n",
    "            random.shuffle(array)\n",
    "            for j in range(7):\n",
    "                print(str(j)+\"    \"+str(array[j]))\n",
    "            print()\n",
    "            g = input()\n",
    "            if array[int(g)] == intruder:\n",
    "                successList.append(1)\n",
    "                print(\"success!\")\n",
    "            else:\n",
    "                successList.append(0)\n",
    "                print(\"fail :(\")\n",
    "\n",
    "    #print(successList)\n",
    "    resultsList.append(successList)\n",
    "print(resultsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(resultsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
