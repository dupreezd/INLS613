{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "text = pd.read_csv(\"all_opinions.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "issues = pd.read_csv(\"database.csv\",encoding='ISO-8859-1')\n",
    "\n",
    "privacyCaseSet = set([])\n",
    "#for each row in the issues column of issues csv\n",
    "issuesFile = open(\"database.csv\")\n",
    "issuesReader = csv.reader(issuesFile, delimiter=',')\n",
    "for row in issuesReader:\n",
    "    if row[39]:\n",
    "    #if the integers value matches 30040, 50010, 50020, 50030, or 50040 (floats)\n",
    "        privacyCaseSet.add(row[0])\n",
    "        #add case_id value of same row to set\n",
    "#print(privacyCaseSet)\n",
    "#print(len(privacyCaseSet))\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "\n",
    "csv.field_size_limit(100000000)\n",
    "opinionsFile = open(\"all_opinions.csv\")\n",
    "opinionsReader = csv.reader(opinionsFile, delimiter=',')\n",
    "dictionary = {}\n",
    "count = 0\n",
    "\n",
    "for r in opinionsReader:   \n",
    "    \n",
    "    if r[9] in privacyCaseSet:\n",
    "        #count += 1 \n",
    "        if r[9]+'-01' in dictionary.keys():\n",
    "            if r[9]+'-02' in dictionary.keys():\n",
    "                if r[9]+'-03' in dictionary.keys():\n",
    "                    if r[9]+'-04' in dictionary.keys():\n",
    "                        dictionary[r[9]+'-05'] = r[-1]\n",
    "                    else: dictionary[r[9]+'-04'] = r[-1]\n",
    "                else: dictionary[r[9]+'-03'] = r[-1]\n",
    "            else: dictionary[r[9]+'-02'] = r[-1]\n",
    "        else: dictionary[r[9]+'-01'] = r[-1]     \n",
    "            \n",
    "\n",
    "#print(len(dictionary))\n",
    "#print(count)\n",
    "\n",
    "with open('dict2.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "   # writer.writerow(['id','text'])\n",
    "    for key, value in dictionary.items():\n",
    "        writer.writerow([key, value])\n",
    "        \n",
    "test = pd.read_csv('dict2.csv',error_bad_lines=False,encoding='ISO-8859-1',names=['id','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data by years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourSix = {}\n",
    "sevenNine = {}\n",
    "nineTwenty = {}\n",
    "\n",
    "for key, value in dictionary.items():\n",
    "    if key.startswith(str(194)):\n",
    "        fourSix.update({key : value})\n",
    "    if key.startswith(str(195)):\n",
    "        fourSix.update({key : value})\n",
    "    if key.startswith(str(196)):\n",
    "        fourSix.update({key : value})\n",
    "    if key.startswith(str(197)):\n",
    "        sevenNine.update({key : value})\n",
    "    if key.startswith(str(198)):\n",
    "        sevenNine.update({key : value})\n",
    "    if key.startswith(str(199)):\n",
    "        nineTwenty.update({key : value})\n",
    "    if key.startswith(str(200)):\n",
    "        nineTwenty.update({key : value})\n",
    "    if key.startswith(str(201)):\n",
    "        nineTwenty.update({key : value})\n",
    "        \n",
    "testList1 = []\n",
    "\n",
    "for entry1 in fourSix.values():\n",
    "    testTokens1 = nltk.word_tokenize(entry1)\n",
    "    if \"privacy\" in testTokens1:\n",
    "        testList1.append(entry1)\n",
    "\n",
    "#print(len(testList1))\n",
    "\n",
    "\n",
    "testList2 = []\n",
    "\n",
    "for entry2 in sevenNine.values():\n",
    "    testTokens2 = nltk.word_tokenize(entry2)\n",
    "    if \"privacy\" in testTokens2:\n",
    "        testList2.append(entry2)\n",
    "\n",
    "#print(len(testList2))\n",
    "\n",
    "\n",
    "testList3 = []\n",
    "\n",
    "for entry3 in nineTwenty.values():\n",
    "    testTokens3 = nltk.word_tokenize(entry3)\n",
    "    if \"privacy\" in testTokens3:\n",
    "        testList3.append(entry3)\n",
    "\n",
    "#print(len(testList3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_topics = 15\n",
    "#lda = LatentDirichletAllocation(n_components=num_topics, max_iter=500, learning_method='online', learning_offset=50.,random_state=1).fit(tf_data_samples)\n",
    "\n",
    "#lda.score(tf_data_samples)\n",
    "\n",
    "from IPython.display import display\n",
    "def print_topics(model, vectorizer, top_n=10):    #### This is the code to print the topics. top_n can be changed. \n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "#print_topics(lda,tf_vectorizer)\n",
    "\n",
    "num_features = 100\n",
    "tf_vectorizer = CountVectorizer(max_df=.25, min_df=.05, max_features=num_features, stop_words='english')\n",
    "\n",
    "yearDfs2 = [testList1, testList2, testList3]\n",
    "ldaList = []\n",
    "\n",
    "for year in yearDfs2:\n",
    "    #n_samples = 200\n",
    "    #data_samples = random.sample(year.text.tolist(), n_samples)\n",
    "    data_samples = year\n",
    "    tf_data_samples = tf_vectorizer.fit_transform(data_samples) \n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    \n",
    "    num_topics = 8\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=500, learning_method='online', learning_offset=50.,random_state=1).fit(tf_data_samples)\n",
    "    lda.score(tf_data_samples)\n",
    "    \n",
    "    ldaList.append(lda)\n",
    "    \n",
    "#   print(lda.score(tf_data_samples))\n",
    "#   print_topics(lda,tf_vectorizer)\n",
    "    \n",
    "#    print()\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Intrustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "0    injunction\n",
      "1    false\n",
      "2    religious\n",
      "3    family\n",
      "4    ___\n",
      "5    testing\n",
      "6    liability\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explore fragment the data, optimize variables see what we get\n",
    "#test our best results with word intrusion\n",
    "#see which topics are different between time periods\n",
    "\n",
    "for ldaItem in ldaList:\n",
    "    success = 0\n",
    "    for idx1, topic1 in enumerate(ldaItem.components_):\n",
    "\n",
    "            print(\"Topic %d:\" % (idx1))\n",
    "            array = [(tf_vectorizer.get_feature_names()[j])\n",
    "                            for j in topic1.argsort()[:-10 - 1:-1]]\n",
    "\n",
    "            cArray = [(tf_vectorizer.get_feature_names()[j])\n",
    "                             for j in topic1.argsort()[:-50 - 1:-1]]\n",
    "\n",
    "            set1 = []\n",
    "            for c in array:\n",
    "                set1.append(c)\n",
    "\n",
    "            array = random.sample(array, 6)\n",
    "\n",
    "            intruder = \"?\"\n",
    "\n",
    "            for idx2, topic2 in enumerate(ldaItem.components_):\n",
    "                if idx2 == idx1:\n",
    "                    continue\n",
    "                else:\n",
    "                    bArray = [(tf_vectorizer.get_feature_names()[k])\n",
    "                            for k in topic2.argsort()[:-10 - 1:-1]]\n",
    "                    bArray = random.sample(bArray, 1)\n",
    "                    if bArray in set1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        array.append(bArray[0])\n",
    "                        intruder = bArray[0]\n",
    "                        break\n",
    "\n",
    "            random.shuffle(array)\n",
    "            for j in range(7):\n",
    "                print(str(j)+\"    \"+str(array[j]))\n",
    "            print()\n",
    "            g = input()\n",
    "            if array[int(g)] == intruder:\n",
    "                success += 1;\n",
    "                print(\"success!\")\n",
    "            else:\n",
    "                print(\"fail :(\")\n",
    "\n",
    "    print(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
